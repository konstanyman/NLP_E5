{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import datasets\n",
    "import torch\n",
    "import sklearn.feature_extraction\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows an example found in GitHub: TurkuNLP/intro-to-nlp/mlp_imdb_hf_dset_and_trainer.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load imdb dataset\n",
    "ds = datasets.load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# print structure of dataset\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can remove the 'unsupervised' set and then shuffle the dataset\n",
    "del ds[\"unsupervised\"]\n",
    "ds = ds.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True, max_features=20000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True, max_features=20000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(binary=True, max_features=20000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a vectorizer (20000 most common words)\n",
    "vectorizer=sklearn.feature_extraction.text.CountVectorizer(binary=True,max_features=20000)\n",
    "\n",
    "# convert texts from the dataset to a list\n",
    "texts=[example[\"text\"] for example in ds[\"train\"]]\n",
    "\n",
    "# fit the vectorizer to train it on our data\n",
    "vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([   10,    58,    80,   786,   887,  1207,  1702,  1919,  2295,\n",
      "        2604,  2672,  3050,  3335,  3516,  3599,  4640,  5529,  5991,\n",
      "        6283,  6370,  6612,  6878,  7127,  7801,  7932,  8241,  8553,\n",
      "        8809,  9602,  9630,  9631, 10392, 10399, 10475, 10558, 10695,\n",
      "       11081, 11116, 11190, 11681, 11832, 12030, 12202, 12363, 12504,\n",
      "       13093, 13209, 13778, 14338, 14340, 14782, 15551, 15763, 15791,\n",
      "       15835, 16020, 17053, 17635, 17897, 17947, 17968, 17982, 18115,\n",
      "       18562, 19107, 19112, 19377, 19398, 19712, 19807])}\n"
     ]
    }
   ],
   "source": [
    "def vectorize_example(ex):\n",
    "    vectorized=vectorizer.transform([ex[\"text\"]]) # [...] makes a \"list\" of the words of the review because the vectorizer expects a list/iterable over inputs, not one input\n",
    "    non_zero_features=vectorized.nonzero()[1] #.nonzero gives a pair of (rows,columns), we want the columns\n",
    "    non_zero_features+=1 #feature index 0 will have a special meaning\n",
    "                         # so let us not produce it by adding +1 to everything\n",
    "    return {\"input_ids\":non_zero_features}\n",
    "\n",
    "vectorized=vectorize_example(ds[\"train\"][0])\n",
    "print(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('10, 1930, 1950, although, and, as, be, bicycle, br, but, call, chaplin, '\n",
      " 'classed, cold, comedy, deals, drama, emotional, ettore, except, fairy, film, '\n",
      " 'for, good, great, hard, hilarious, humanism, is, it, italian, less, letdown, '\n",
      " 'like, little, lovable, masterpiece, matters, meant, more, music, neo, not, '\n",
      " 'of, or, person, picture, probably, realism, realistic, resist, scola, '\n",
      " 'sentimental, serious, sex, should, stone, tale, the, thief, this, though, '\n",
      " 'to, typical, version, very, warmth, was, with, wouldn')\n"
     ]
    }
   ],
   "source": [
    "# map indexes back to words to check that everything works\n",
    "\n",
    "idx2word=dict((i,w) for (w,i) in vectorizer.vocabulary_.items()) #inverse the vocab dictionary\n",
    "words=[]\n",
    "for idx in vectorized[\"input_ids\"]:\n",
    "    words.append(idx2word[idx-1]) ## It is easy to forgot we moved all by +1\n",
    "pprint(\", \".join(words)) #This is now the bag of words representation of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [10,\n",
      "               58,\n",
      "               80,\n",
      "               786,\n",
      "               887,\n",
      "               1207,\n",
      "               1702,\n",
      "               1919,\n",
      "               2295,\n",
      "               2604,\n",
      "               2672,\n",
      "               3050,\n",
      "               3335,\n",
      "               3516,\n",
      "               3599,\n",
      "               4640,\n",
      "               5529,\n",
      "               5991,\n",
      "               6283,\n",
      "               6370,\n",
      "               6612,\n",
      "               6878,\n",
      "               7127,\n",
      "               7801,\n",
      "               7932,\n",
      "               8241,\n",
      "               8553,\n",
      "               8809,\n",
      "               9602,\n",
      "               9630,\n",
      "               9631,\n",
      "               10392,\n",
      "               10399,\n",
      "               10475,\n",
      "               10558,\n",
      "               10695,\n",
      "               11081,\n",
      "               11116,\n",
      "               11190,\n",
      "               11681,\n",
      "               11832,\n",
      "               12030,\n",
      "               12202,\n",
      "               12363,\n",
      "               12504,\n",
      "               13093,\n",
      "               13209,\n",
      "               13778,\n",
      "               14338,\n",
      "               14340,\n",
      "               14782,\n",
      "               15551,\n",
      "               15763,\n",
      "               15791,\n",
      "               15835,\n",
      "               16020,\n",
      "               17053,\n",
      "               17635,\n",
      "               17897,\n",
      "               17947,\n",
      "               17968,\n",
      "               17982,\n",
      "               18115,\n",
      "               18562,\n",
      "               19107,\n",
      "               19112,\n",
      "               19377,\n",
      "               19398,\n",
      "               19712,\n",
      "               19807],\n",
      " 'label': 1,\n",
      " 'text': \"This film is like a 1950-version of Ettore Scola's Brutti sporchi e \"\n",
      "         'cattivi. Less sex and less realism, but a tale with great humanism '\n",
      "         \"and warmth. I wouldn't call this a neo-realistic picture. It's very \"\n",
      "         'sentimental and more like a fairy tale, and should probably be '\n",
      "         'classed as a comedy, although it deals with serious matters (a '\n",
      "         'little like Chaplin or 1930-comedy). Typical Italian though, very '\n",
      "         'emotional, and hard to resist except for a stone cold person. The '\n",
      "         'sentimentalism is a letdown, although this picture was not meant to '\n",
      "         \"be a realistic drama. It's not a masterpiece like Umberto D or The \"\n",
      "         'Bicycle Thief. But it is a lovable and hilarious comedy, with good '\n",
      "         'music.<br /><br />7/10'}\n"
     ]
    }
   ],
   "source": [
    "# apply the tokenizer to the whole dataset using .map()\n",
    "ds_tokenized = ds.map(vectorize_example)\n",
    "pprint(ds_tokenized[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label and text remain in every example but in addition we now have 'input_ids' for every example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a collator function that recieves a list of examples and returns a batch with padded examples\n",
    "\n",
    "def collator(list_of_examples):\n",
    "    batch={\"labels\":torch.tensor(list(ex[\"label\"] for ex in list_of_examples))} #assign label values to batch\n",
    "\n",
    "    # pad examples\n",
    "    tensors = []\n",
    "    max_len = max(len(example[\"input_ids\"]) for example in list_of_examples)\n",
    "    for example in list_of_examples:\n",
    "        ids=torch.tensor(example[\"input_ids\"]) #pick the input ids\n",
    "        padded=torch.nn.functional.pad(ids, (0, max_len-ids.shape[0])) #pad by max - current length, pads with zero by default\n",
    "        tensors.append(padded) #accumulated the padded ids\n",
    "    batch[\"input_ids\"]=torch.vstack(tensors) #now that we have all of them the same length, a simple vstack() stacks them up\n",
    "    return batch #...and that's all there is to it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
